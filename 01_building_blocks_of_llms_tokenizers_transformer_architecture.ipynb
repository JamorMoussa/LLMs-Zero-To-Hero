{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 01. Building Blocks of LLMs: Tokenizers & Transformer Architecture"
      ],
      "metadata": {
        "id": "ZMnfhjduE8vX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this chapter, we're going to cover:\n",
        "\n",
        "- The building blocks of language modeling, from concepts like tokenization and token embeddings to constructing the entire Transformer architecture from scratch.\n",
        "\n",
        "- Discovering the Hugging Face APIs for tokenization, text generation, and more with some pre-trained LLMs out there, such as GPT-2 and LLaMA.\n",
        "\n",
        "> **Mini-Project**:\n",
        ">\n",
        "> Train a custom tokenizer (Hugging Face API compatible) for Moroccan Darija. We'll use the DODa dataset [https://github.com/darija-open-dataset/dataset](https://github.com/darija-open-dataset/dataset). The tokenizer should support both English and Moroccan Darija."
      ],
      "metadata": {
        "id": "TUPyimRULP_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction To Tokenization"
      ],
      "metadata": {
        "id": "jXnqAx7NPMbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 What's Tokenization?"
      ],
      "metadata": {
        "id": "tjWSeKDnb4Kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What's tokenization and what is it used for?**  \n",
        "\n",
        "Well, how can we feed text into language models? LLMs are essentially large neural networks (commonly Transformer-based) that can only process numbers. So, we need a process that converts strings into numbers. The answer is **Tokenization**, which is the process of splitting raw text into small units called **tokens**. These tokens are then mapped to **IDs**, and all available tokens along with their **IDs** form a **Vocabulary**.  \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/JamorMoussa/LLMs-Zero-To-Hero/refs/heads/main/assets/01/tokenizer.png\"/>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pfEHASh2Ppwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three main types of tokenizers used in ðŸ¤— Hugging Face Transformers: **Byte-Pair Encoding (BPE)**, **WordPiece**, and **SentencePiece**, which we will cover in the following sections."
      ],
      "metadata": {
        "id": "3xufFiwWXhOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Challenges"
      ],
      "metadata": {
        "id": "SdWyDLrpcaE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizers** are trained on huge datasets to determine efficient splitting. This is a crucial stepâ€”**Andrej Karpathy** mentioned that bad tokenization can lead to significant issues for LLMs.  \n",
        "\n",
        "> Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.  \n",
        ">   \n",
        "> - Why can't LLM spell words? Tokenization.  \n",
        "> - Why can't LLM do super simple string processing tasks like reversing a string? Tokenization.  \n",
        "> - Why is LLM worse at non-English languages (e.g., Japanese)? Tokenization.  \n",
        "> - Why is LLM bad at simple arithmetic? Tokenization.  \n",
        "> - Why did GPT-2 have more than necessary trouble coding in Python? Tokenization.  "
      ],
      "metadata": {
        "id": "vx5-XYqCcYf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸš§ **Tokenization is not as easy as it seems!**  \n",
        "\n",
        "While **character tokenization** is very simple and results in a minimal vocabulary, it makes it harder for the model to learn meaningful input representations.  \n",
        "\n",
        "ðŸ“š On the other hand, **word tokenization** (i.e., splitting text by words) leads to a very large vocabulary size, ðŸ“ˆ increasing computational complexity due to the embedding matrix becoming too large.  \n",
        "\n",
        "âœ¨ To get the best of both worlds, Transformer models use a hybrid approach between word-level and character-level tokenization called **subword tokenization**.  \n",
        "\n",
        "ðŸ’¡ **Subword tokenization** allows models to have a resonable volcabulary size, while being able to learn text representation."
      ],
      "metadata": {
        "id": "Xmyka_Sjcgmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Subword Tokenization  \n",
        "\n",
        "**TODO**: Refer to [tokenizer summary ðŸ¤—](https://huggingface.co/docs/transformers/en/tokenizer_summary) and write a section covering the **BPE**, **WordPiece**, and **SentencePiece** techniques.  :\n"
      ],
      "metadata": {
        "id": "hu_IEEbvH4ee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Hugging Face Tokenizer API"
      ],
      "metadata": {
        "id": "V8BEV5KxMUlq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1"
      ],
      "metadata": {
        "id": "ZnI-rtsy_d9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "xWE9n8mhQOac"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304,
          "referenced_widgets": [
            "051ab6806f8740f4b38e2159ba9fc5f7",
            "228aa930813f4a65a889ff339b6687b5",
            "68fa2850478f4044bdb276ce7ce064c2",
            "f70de76cc3654e92bc262d0894da1fbb",
            "3c13d45e562c4864a2cd297f10e64ffe",
            "7a1ff59624f34f74a67453eefd9c1e36",
            "b40a72cee6334bf8908aea6210a82da1",
            "0f6cc335f77a490789ed79c022d25e39",
            "42a28594fc71402783b7788590fee64b",
            "f4e2faae5a2f47c496db5a74002fa1cc",
            "d206ed6abc0a4600aa1d33ea7157a66e",
            "3013ad1187c0419b919c4ff958b48400",
            "f07d5d1de8dc4ea2bfc229c1f16dd73e",
            "76d7966a75db427f887de81395998c98",
            "313190fd304346828fccbfcacfa97858",
            "5cb37b75c3c54067957f7ec96bb28d1c",
            "dc80d9b5fc024b98866290da24d98367",
            "ad6f5958a72b45fd96c3f25aa4c80ed1",
            "3caed73e7fbd452788a29aca5c93dfe1",
            "98d6684e34664696b5c0331cf60fd323",
            "a8c809d7f2e448948b8b55cc3a1c2d2f",
            "ac69c5ac91064efaa77066391ed1d205",
            "edf2fa58a41d4943a5d0d52cc278666e",
            "a9b7ffea09cf4e1f8ec4480c5270323e",
            "8fbab1e531114a239c34bdd56c0d9ba7",
            "8ad3c6e0813a4fc7aaa81b136deae933",
            "c5e2c9bcd92346e3abd47efd3af89177",
            "ff929a5fce354a5db722c0ae91dd80e5",
            "772017f98be543bea21747ebf42671ce",
            "e48250f3a9b149729323565a75738cba",
            "d53033e975e6407ba1e09a29cdf0ab69",
            "b64b1eece43d4515a3c96b59e11cc57b",
            "fe392043c5f64d8c9de203537b4fa093",
            "37e509ddc97843c993dea6d606282eac",
            "e17bb819e1ac48a7ae1856750187a0ba",
            "397ea0e4eac04ad599848537d61e1a22",
            "b7905ad639ee471db176c179965781ef",
            "3aae9c5106dc4d12bc5d17386b924b29",
            "c45f46b2037b4cf18c25a0ec3eb435e9",
            "18dd0859b62e4e75a3417770578aae18",
            "19e6631487c3427ea6b10f8013535ab0",
            "3367a5d1f4b24fa2aa2e521daab17f3b",
            "38af7ebb98244aa3a23d2c10111d4f8e",
            "663a370b37a343bcadb6a321c8e8c4db",
            "04e28d09e87a4912ab5518f57a317d4d",
            "a5201b626b064706be5d26cf0bc6be26",
            "09af73977f8f41d78e14d56dcd063bd8",
            "d25731e1919b4d36b73db5c5462912d0",
            "6981ea35a145431494406706e6384ba1",
            "c3e9a7ae76164d65b307998ac8d44679",
            "7882c77373814b75a3d037447e8dc6f8",
            "63ce411ca8944f929143b1b1c24a71e2",
            "03fdd3bd717a4567b4d6e8d620f85668",
            "4325b752627c46f7abacde2e1b37d0e8",
            "0c1d0dfec5f046819108ab4c1f44a60f"
          ]
        },
        "id": "kYqsXqc2O1O_",
        "outputId": "7716f311-e89c-4b38-a42f-bbcc88278364"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "051ab6806f8740f4b38e2159ba9fc5f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3013ad1187c0419b919c4ff958b48400"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "edf2fa58a41d4943a5d0d52cc278666e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37e509ddc97843c993dea6d606282eac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04e28d09e87a4912ab5518f57a317d4d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Welcome to LLMs zero to Hero Course ðŸ¤—\"\n",
        "\n",
        "pprint(tokenizer(text= text)) # returns 'attenstion_mask' and 'input_ids' as list."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk11cR1fPAgk",
        "outputId": "21cfcc88-5ed5-4761-880c-eca46a643a9c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            " 'input_ids': [14618,\n",
            "               284,\n",
            "               27140,\n",
            "               10128,\n",
            "               6632,\n",
            "               284,\n",
            "               8757,\n",
            "               20537,\n",
            "               12520,\n",
            "               97,\n",
            "               245]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(tokenizer(text= text, return_tensors= \"pt\")) # as Pytorch Tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVW_i1m1P__m",
        "outputId": "7ed83158-78c7-462d-baff-187dd4dc65ae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
            " 'input_ids': tensor([[14618,   284, 27140, 10128,  6632,   284,  8757, 20537, 12520,    97,\n",
            "           245]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"Hello Everyone âœ‹\",\n",
        "    \"Welcome to LLMs Zero-to-Hero Course ðŸ¤—\"\n",
        "]\n",
        "\n",
        "pprint(tokenizer(text= sentences)) # batches of sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC4RQi2-Qut1",
        "outputId": "84dbcf43-7a04-4485-a551-c609241d9133"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
            " 'input_ids': [[15496, 11075, 14519, 233],\n",
            "               [14618,\n",
            "                284,\n",
            "                27140,\n",
            "                10128,\n",
            "                12169,\n",
            "                12,\n",
            "                1462,\n",
            "                12,\n",
            "                30411,\n",
            "                20537,\n",
            "                12520,\n",
            "                97,\n",
            "                245]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(tokenizer(text= sentences, return_tensors= \"pt\"))"
      ],
      "metadata": {
        "id": "GuUUwUxMRmo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/JamorMoussa/LLMs-Zero-To-Hero/refs/heads/main/assets/01/return_tensor_error.png\"/>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zphI7mIrV1v1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The issue is that when converting a batch of sentences into a PyTorch tensor, all sentences must have the same length, which is not the case here.  "
      ],
      "metadata": {
        "id": "IMc8puPV6G6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(text= sentences) # list format\n",
        "\n",
        "input_ids = inputs[\"input_ids\"]\n",
        "for sentence, ids in zip(sentences, input_ids):\n",
        "    print(f\"Sentence: {sentence} - number of tokens: {len(ids)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf3B7B156h4Z",
        "outputId": "33bf458d-2ab7-48d3-af6f-67c18161dfc5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Hello Everyone âœ‹ - number of tokens: 4\n",
            "Sentence: Welcome to LLMs Zero-to-Hero Course ðŸ¤— - number of tokens: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To solve this issue, we need to pad the first sentence from 4 tokens to 13 tokens using a `pad_token`. Since the GPT-2 tokenizer doesn't have a `pad_token` by default, we need to add it ourselves.  "
      ],
      "metadata": {
        "id": "dcF_zlzb7tqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = \"[PAD]\"\n",
        "\n",
        "pprint(tokenizer(text= sentences, padding= True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH1Xyq6X7jzW",
        "outputId": "a265beef-8355-4e0f-97a4-6fd9482a4fb9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attention_mask': [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
            " 'input_ids': [[15496,\n",
            "                11075,\n",
            "                14519,\n",
            "                233,\n",
            "                50256,\n",
            "                50256,\n",
            "                50256,\n",
            "                50256,\n",
            "                50256,\n",
            "                50256,\n",
            "                50256,\n",
            "                50256,\n",
            "                50256],\n",
            "               [14618,\n",
            "                284,\n",
            "                27140,\n",
            "                10128,\n",
            "                12169,\n",
            "                12,\n",
            "                1462,\n",
            "                12,\n",
            "                30411,\n",
            "                20537,\n",
            "                12520,\n",
            "                97,\n",
            "                245]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(text= sentences, return_tensors=\"pt\", padding= True)\n",
        "pprint(inputs)\n",
        "print(f\"shape: {inputs['input_ids'].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG0n7D7Y8gW6",
        "outputId": "069a9b1d-8ede-409c-c3e1-3082540d0921"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
            " 'input_ids': tensor([[15496, 11075, 14519,   233, 50256, 50256, 50256, 50256, 50256, 50256,\n",
            "         50256, 50256, 50256],\n",
            "        [14618,   284, 27140, 10128, 12169,    12,  1462,    12, 30411, 20537,\n",
            "         12520,    97,   245]])}\n",
            "shape: torch.Size([2, 13])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Chat Template"
      ],
      "metadata": {
        "id": "kmHtlngSAERo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "5427cb3ab4f94af5b0aa548ebc8a43dc",
            "9bf6686be7f64641884e8aa118d1cb0e",
            "219e750a440249839382aacfcb195be6",
            "0e8b996e60b84a648efb128082aabe17",
            "b8dfeef1b6c3498e9096014c320de5fd",
            "441174ed11af46058faa05ddc2084c19",
            "1eb53e9844254174991244386b661572",
            "cb13e33a99c745cbb5828e20eddbb367",
            "41f4e64caf9e406b9d90b66f0ef273df",
            "8e5f42a32ed74eccafaf0f36af9fed77",
            "7630c7e94aa54ce5b86a8815acd2cd8e",
            "6c38d3627b4c424583d983a161f24db1",
            "cd6564fae33d40839c944faad3480665",
            "a2bf64784bdf4e90864064d414efd66c",
            "fa3ae7629fb9478b8f002550ff332b3d",
            "8f42cf8263cb498582cdc2508855ddf0",
            "68eccbcc20a54e6f8a2eb5e9a5a2bb8a",
            "69b85bd242e24c3ba870fb1292c0a8c0",
            "ec140b8ec5c74d00a17399b0b0fd8bcc",
            "21ff98989a46477987ab499648499274",
            "749e2a0af43545c289c454e159db5b55",
            "0f0a24a0a088476c9ca3d4ed2bd5235c",
            "2acab0e4264c446dac3629db19d2edea",
            "44c492acb53e4040ad4d1e5658bfe3fa",
            "80c2dd08dbf24266b0fbd86c05c04f11",
            "911421094a0644abb514ef5e6cdc2a5b",
            "d4b00b0ac2704681a1c5690cc5fd1675",
            "78bc06f7864c484d9f724d1d96bab1e8",
            "0f1a42a0530a4cd18974031b35c7e3de",
            "ae875ba44c6f4d8cad5c2d47de6a58f4",
            "ab88be140a5f4105a60a358035dc0862",
            "0493b631091d4c6da1f48d78e75ea4bf",
            "7fed27e559be4eb2ba86c47d5f994740",
            "f4606a78d796432e887da554d2b5a96b",
            "3755dfb0ef9a4c94b5c2dc334f257354",
            "ef0b2499f19f49569c1d44209cafde49",
            "21a0eec4981a4d369b259f196fd5adbc",
            "32a28dd6748542879d2416fb7294fe3b",
            "779aa52d69f14451a3c50ccddc7ee561",
            "8cf6ef3b481f450bb61014f6b51b0d4e",
            "588485a0fce74984b0553a01da992778",
            "a4afec4cb34c43809d0fc467e266a057",
            "dffa3af37ce245f2ac5f7df73427ed8d",
            "555cc0b4351949758feb86c05a5c1665",
            "b7894be4988445f09675a3a72a0b57ef",
            "0c3cd5582cf44638935a0dc222aed3e1",
            "4300517a7510494c8d0f1d1647888d63",
            "36ff14ac408b47048f6603a7fcb9e06e",
            "f3b1d4de2b4a46828d5066f6b9484560",
            "75fb05bac7fb466a929f833d6275ddf4",
            "9fe6c5dedcf34d5bbdf14166141109e0",
            "08d884091f584400bb090b38c5c5efb1",
            "d06079c02a3244e7bed69753480f5fdd",
            "41fd6701a10e4b2282e26548cec3d1d0",
            "ba81cc7c09fb47fdab7e97a8621b252e"
          ]
        },
        "id": "jUodIGCBA79Z",
        "outputId": "572a2470-2870-4e7e-a4e0-bb4b078d8660"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5427cb3ab4f94af5b0aa548ebc8a43dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c38d3627b4c424583d983a161f24db1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2acab0e4264c446dac3629db19d2edea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4606a78d796432e887da554d2b5a96b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7894be4988445f09675a3a72a0b57ef"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHLipvQqBDxt",
        "outputId": "c8de0217-619b-42d7-ebda-94d12ef73edb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaTokenizerFast(name_or_path='HuggingFaceH4/zephyr-7b-beta', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>', 'additional_special_tokens': ['<unk>', '<s>', '</s>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
              "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"How many helicopters can a human eat in one sitting?\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "GretM9268m81"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(\n",
        "    tokenizer.apply_chat_template(\n",
        "        conversation= messages, return_tensors= \"pt\", tokenize= False,\n",
        "        add_generation_prompt= True, # to start generation: '<|assistant|>\\n'\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWaj3QN--2w_",
        "outputId": "bb9bf427-3c59-4c83-d6da-f1cc506f7495"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('<|system|>\\n'\n",
            " 'You are a friendly chatbot who always responds in the style of a pirate</s>\\n'\n",
            " '<|user|>\\n'\n",
            " 'How many helicopters can a human eat in one sitting?</s>\\n'\n",
            " '<|assistant|>\\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "    conversation= messages, return_tensors= \"pt\", add_generation_prompt=True\n",
        ")\n",
        "\n",
        "pprint(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH463BlqCllo",
        "outputId": "673ac491-b65e-44fa-cb79-930babc9e131"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  523, 28766,  6574, 28766, 28767,    13,  1976,   460,   264, 10131,\n",
            "         10706, 10093,   693,  1743,  2603,  3673,   297,   272,  3238,   302,\n",
            "           264, 17368,   380,     2, 28705,    13, 28789, 28766,  1838, 28766,\n",
            "         28767,    13,  5660,  1287, 19624,   410,  1532,   541,   264,  2930,\n",
            "          5310,   297,   624,  6398, 28804,     2, 28705,    13, 28789, 28766,\n",
            "           489, 11143, 28766, 28767,    13]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(inputs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "htl3dnFbETZL",
        "outputId": "2a275c36-081b-4bd9-8911-fe9cee04e267"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|system|>\\nYou are a friendly chatbot who always responds in the style of a pirate</s> \\n<|user|>\\nHow many helicopters can a human eat in one sitting?</s> \\n<|assistant|>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        conversation= messages, add_generation_prompt=True, tokenize= False),\n",
        "    return_tensors= \"pt\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7-PEZS4EvFy",
        "outputId": "fe2800d6-cd98-4cc9-9a78-0a6b858883ff"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    1,   523, 28766,  6574, 28766, 28767,    13,  1976,   460,   264,\n",
              "         10131, 10706, 10093,   693,  1743,  2603,  3673,   297,   272,  3238,\n",
              "           302,   264, 17368,   380,     2, 28705,    13, 28789, 28766,  1838,\n",
              "         28766, 28767,    13,  5660,  1287, 19624,   410,  1532,   541,   264,\n",
              "          2930,  5310,   297,   624,  6398, 28804,     2, 28705,    13, 28789,\n",
              "         28766,   489, 11143, 28766, 28767,    13]])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4oP2lbTNE5b6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
