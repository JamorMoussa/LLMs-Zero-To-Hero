{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 01. Building Blocks of LLMs: Tokenizers & Transformer Architecture"
      ],
      "metadata": {
        "id": "ZMnfhjduE8vX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this chapter, we're going to cover:\n",
        "\n",
        "- The building blocks of language modeling, from concepts like tokenization and token embeddings to constructing the entire Transformer architecture from scratch.\n",
        "\n",
        "- Discovering the Hugging Face APIs for tokenization, text generation, and more with some pre-trained LLMs out there, such as GPT-2 and LLaMA.\n",
        "\n",
        "> **Mini-Project**:\n",
        ">\n",
        "> Train a custom tokenizer (Hugging Face API compatible) for Moroccan Darija. We'll use the DODa dataset [https://github.com/darija-open-dataset/dataset](https://github.com/darija-open-dataset/dataset). The tokenizer should support both English and Moroccan Darija."
      ],
      "metadata": {
        "id": "TUPyimRULP_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Tokenization"
      ],
      "metadata": {
        "id": "jXnqAx7NPMbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> references: https://huggingface.co/docs/transformers/en/tokenizer_summary"
      ],
      "metadata": {
        "id": "4slj13MmdiK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.1 What's Tokenization?"
      ],
      "metadata": {
        "id": "tjWSeKDnb4Kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **What's tokenization and what is it used for?**  \n",
        "\n",
        "Well, how can we feed text into language models? LLMs are essentially large neural networks (commonly Transformer-based) that can only process numbers. So, we need a process that converts strings into numbers. The answer is **Tokenization**, which is the process of splitting raw text into small units called **tokens**. These tokens are then mapped to **IDs**, and all available tokens along with their **IDs** form a **Vocabulary**.  \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/JamorMoussa/LLMs-Zero-To-Hero/refs/heads/main/assets/01/tokenizer.png\"/>\n",
        "\n"
      ],
      "metadata": {
        "id": "pfEHASh2Ppwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three main types of tokenizers used in ðŸ¤— Hugging Face Transformers: **Byte-Pair Encoding (BPE)**, **WordPiece**, and **SentencePiece**, which we will cover in the following sections."
      ],
      "metadata": {
        "id": "3xufFiwWXhOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.2 Challenges"
      ],
      "metadata": {
        "id": "SdWyDLrpcaE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizers** are trained on huge datasets to determine efficient splitting. This is a crucial stepâ€”**Andrej Karpathy** mentioned that bad tokenization can lead to significant issues for LLMs.  \n",
        "\n",
        "> Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.  \n",
        ">   \n",
        "> - Why can't LLM spell words? Tokenization.  \n",
        "> - Why can't LLM do super simple string processing tasks like reversing a string? Tokenization.  \n",
        "> - Why is LLM worse at non-English languages (e.g., Japanese)? Tokenization.  \n",
        "> - Why is LLM bad at simple arithmetic? Tokenization.  \n",
        "> - Why did GPT-2 have more than necessary trouble coding in Python? Tokenization.  "
      ],
      "metadata": {
        "id": "vx5-XYqCcYf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸš§ **Tokenization is not as easy as it seems!**  \n",
        "\n",
        "ðŸ”¤ While **character tokenization** is very simple and results in a minimal vocabulary, it makes it harder for the model to learn meaningful input representations.  \n",
        "\n",
        "ðŸ“š On the other hand, **word tokenization** (i.e., splitting text by words) leads to a very large vocabulary size, ðŸ“ˆ increasing computational complexity due to the embedding matrix becoming too large.  \n",
        "\n",
        "âœ¨ To get the best of both worlds, Transformer models use a hybrid approach between word-level and character-level tokenization called **subword tokenization**.  "
      ],
      "metadata": {
        "id": "Xmyka_Sjcgmp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhWzi1bcEvWb"
      },
      "outputs": [],
      "source": []
    }
  ]
}